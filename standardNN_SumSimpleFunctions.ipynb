{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Convex/Standard Neural Network**\n",
        "\n",
        "sum simple functions with real numbers"
      ],
      "metadata": {
        "id": "tas7uZNG6mvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "LLsxucjt6lkI"
      },
      "outputs": [],
      "source": [
        "# import relevant libraries, numpy, pandas, tensorflow, datasets\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_convex_loss = lambda x, y: tf.keras.activations.tanh(tf.math.square(x - y))\n",
        "# example of a non convex function\n"
      ],
      "metadata": {
        "id": "g5jEVyb4hs3o"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model and instantiating layers\n",
        "\n",
        "# NN model using convex functions\n",
        "model1 = Sequential([\n",
        "    Dense(units=4, input_dim=5, activation='relu'),\n",
        "    Dense(units=1, input_dim=1, activation=\"linear\")\n",
        "])\n",
        "\n",
        "# NN model using non convex functions (for activation and loss)\n",
        "model2 = Sequential([\n",
        "    Dense(units=4, input_dim=5, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
        "    Dense(units=1, input_dim=1, activation=tf.keras.layers.PReLU(alpha_initializer='zeros'))\n",
        "])\n",
        "\n",
        "model1.summary()\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "Mz3-l5Id-Wn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f658b4-70f7-42c7-fce6-84e723ab08ad"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_52 (Dense)            (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29 (116.00 Byte)\n",
            "Trainable params: 29 (116.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_54 (Dense)            (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 1)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30 (120.00 Byte)\n",
            "Trainable params: 30 (120.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate random data, arrays with numbers\n",
        "\n",
        "np.random.seed(36)\n",
        "\n",
        "X1_train = np.random.rand(100, 5)\n",
        "X2_train = np.random.rand(100, 5)\n",
        "\n",
        "Y1_train = np.sum(X1_train, axis=1)\n",
        "Y2_train = np.sum(X2_train, axis=1)\n",
        "\n",
        "\n",
        "print(\"Training data for model 1: \", X1_train.shape, \" \", X2_train.shape , \"\\nTraining data for model 2: \" , Y1_train.shape , \"   \", Y2_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqP9Xh8S63AH",
        "outputId": "1b0c23a3-bc59-4814-bca0-31b05c0f7547"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data for model 1:  (100, 5)   (100, 5) \n",
            "Training data for model 2:  (100,)     (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile models\n",
        "\n",
        "model1.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model2.compile(optimizer='adam', loss=non_convex_loss)\n",
        "\n",
        "\n",
        "# train models\n",
        "\n",
        "model1.fit(X1_train, Y1_train, epochs=100, batch_size=10, shuffle=True)\n",
        "\n",
        "model2.fit(X2_train, Y2_train, epochs=100, batch_size=10, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVmaeHVVEmBe",
        "outputId": "bb23cf89-ddca-4670-f57a-658ce4d73c1c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 3ms/step - loss: 7.0012\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.8095\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.6556\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.5075\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.3801\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.2622\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.1488\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.0336\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 5.9195\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 5.7887\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 5.6279\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.4211\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.1785\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9060\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.6326\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.3590\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.1045\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.8542\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.6172\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.3875\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.1622\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.9497\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.7424\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.5421\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.3517\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.1636\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.9879\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8131\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.6416\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.4852\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.3293\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.1818\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.0416\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9136\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7954\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6789\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5833\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4951\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4178\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3516\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2969\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2465\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2091\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1781\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1535\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1333\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1195\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1079\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0996\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0931\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0887\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0852\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0832\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0815\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0797\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0792\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0781\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0774\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0769\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0764\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0759\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0755\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0752\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0746\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0742\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0739\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0734\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0731\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0726\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0722\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0719\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0716\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0712\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0706\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0703\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0699\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0695\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0691\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0687\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0683\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0679\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0675\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0671\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0667\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0664\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0660\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0656\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0652\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0648\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0644\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0641\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0636\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0632\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0629\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0626\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0621\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0617\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0614\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0611\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0606\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 2ms/step - loss: 0.9936\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9932\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9928\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9921\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9914\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9908\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9896\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9878\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9847\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9815\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9758\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9687\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9562\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9371\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9039\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8452\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7618\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6482\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5109\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3679\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2421\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1524\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0914\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0594\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0444\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0378\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0360\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0347\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0343\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0340\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0336\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0334\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0329\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0326\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0322\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0318\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0314\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0312\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0308\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0303\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0300\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0297\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0293\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0290\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0286\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0283\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0279\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0276\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0272\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0269\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0265\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0261\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0258\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0255\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0251\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0248\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0243\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0240\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0236\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0233\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0228\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0225\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0222\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0219\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0214\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0212\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0207\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0204\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0201\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0197\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0194\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0191\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0188\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0182\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0179\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0177\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0166\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0164\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0162\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0159\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0156\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0154\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0152\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0149\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0145\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0143\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0139\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0136\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0134\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0132\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0131\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dd75c077220>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new random data for prediction, to test models\n",
        "\n",
        "X1_test = np.random.rand(10, 5)\n",
        "\n",
        "X2_test = np.random.rand(10, 5)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "cpredictions = model1.predict(X1_test)\n",
        "\n",
        "npredictions = model2.predict(X2_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kBt3R7yGZh6",
        "outputId": "9e5dcdaf-f91f-4086-fd2f-12f4c9f2b146"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test results:\n",
        "\n",
        "# Actual sums for test data\n",
        "\n",
        "y1_test_actual = np.sum(X1_test, axis=1)\n",
        "y2_test_actual = np.sum(X2_test, axis=1)\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "cpredictions = model1.predict(X1_test).flatten()\n",
        "npredictions = model2.predict(X2_test).flatten()\n",
        "# Flatten to make it a 1D array\n",
        "\n",
        "# Calculate mean squared error\n",
        "\n",
        "mse = mean_squared_error(y1_test_actual, cpredictions)\n",
        "\n",
        "#Calculating mean absolute error\n",
        "\n",
        "non_convex = non_convex_loss(y2_test_actual, npredictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd0wxij_RsXI",
        "outputId": "bf036b80-6ae8-422c-cd2c-3b9aefa988bb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking test results:\n",
        "\n",
        "# Print actual sums, predicted sums, and MSE\n",
        "\n",
        "print(\"\\nActual Sum  |  Predicted Sum\")\n",
        "print(\"---------------------------\")\n",
        "for i in range(len(y1_test_actual)):\n",
        "    print(f\"{y1_test_actual[i]:.3f}      |  {cpredictions[i]:.3f}\")\n",
        "\n",
        "print(f\"\\nmean_Squared_error: {mse:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print actual sums, predicted sums, and MAE\n",
        "\n",
        "print(\"Actual Sum  |  Predicted Sum\")\n",
        "print(\"---------------------------\")\n",
        "for i in range(len(y2_test_actual)):\n",
        "    print(f\"{y2_test_actual[i]:.3f}      |  {npredictions[i]:.3f}\")\n",
        "\n",
        "print(f\"\\nnon convex loss function: {np.mean(non_convex)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSZuxdvsG0-6",
        "outputId": "48f6fcd6-9ac8-46db-e6d3-d6da5c756674"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Actual Sum  |  Predicted Sum\n",
            "---------------------------\n",
            "3.433      |  3.148\n",
            "1.580      |  1.884\n",
            "3.054      |  2.721\n",
            "3.293      |  3.183\n",
            "2.640      |  2.240\n",
            "1.356      |  1.721\n",
            "2.879      |  2.637\n",
            "2.440      |  2.216\n",
            "1.967      |  2.337\n",
            "2.234      |  2.304\n",
            "\n",
            "mean_Squared_error: 0.084\n",
            "Actual Sum  |  Predicted Sum\n",
            "---------------------------\n",
            "3.100      |  3.074\n",
            "0.870      |  1.245\n",
            "1.979      |  2.139\n",
            "2.530      |  2.550\n",
            "0.916      |  1.209\n",
            "0.890      |  1.198\n",
            "2.391      |  2.350\n",
            "2.342      |  2.348\n",
            "3.220      |  3.144\n",
            "3.166      |  3.042\n",
            "\n",
            "non convex loss function: 0.03691647334510142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for MSE is\n",
        "\n",
        "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$.\n",
        "\n"
      ],
      "metadata": {
        "id": "XTyE3mAfWHq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "- if summing negative values then it would be preferable to not use ReLu in order to not lose information (I think)\n"
      ],
      "metadata": {
        "id": "8PeUZ4U9l5pV"
      }
    }
  ]
}